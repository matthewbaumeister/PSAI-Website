# Persistent Scraper Behavior - Never Skip Pages!

## âœ… **NEW BEHAVIOR (What You Wanted):**

### **Core Rule:**
**Never skip a page. Only move to next day when we find the natural end (page with <100 contracts).**

---

## ðŸ“Š **How It Works Now:**

### **Example Scenario:**

```
Day: 2025-10-29

Page 1:  âŒ Fail â†’ Cool 30s â†’ Retry
Page 1:  âŒ Fail â†’ Cool 1min â†’ Retry
Page 1:  âŒ Fail â†’ Cool 2min â†’ Retry
Page 1:  âŒ Fail â†’ Cool 4min â†’ Retry
Page 1:  âœ… SUCCESS! â†’ 100 contracts

Page 2:  âœ… SUCCESS! â†’ 100 contracts

Page 3:  âŒ Fail â†’ Cool 30s â†’ Retry
Page 3:  âœ… SUCCESS! â†’ 100 contracts

...keep going...

Page 11: âœ… SUCCESS! â†’ 71 contracts â† FOUND END!

Result: Day complete! All data captured.
```

---

## ðŸ”„ **Retry Strategy:**

### **Exponential Backoff:**

| Attempt | Cooldown | Cumulative Time |
|---------|----------|-----------------|
| 1 | 0s | 0s |
| 2 | 30s | 30s |
| 3 | 1min | 1m 30s |
| 4 | 2min | 3m 30s |
| 5 | 4min | 7m 30s |
| 6+ | 5min (max) | Adds 5min per attempt |
| 20 (final) | 5min | ~80 minutes total |

**Rationale:** 
- Start fast (30s) for quick API hiccups
- Scale up for sustained API issues
- Cap at 5min to avoid infinite waits
- 20 attempts = ~80 minutes per page (very persistent!)

---

## ðŸŽ¯ **When Does It Move to Next Day?**

### **Only 2 Scenarios:**

1. **Natural End (GOOD):**
   ```
   Page 11: Found 71 contracts (< 100)
   â†’ âœ… Day complete! Move to previous day.
   ```

2. **Complete API Failure (RARE):**
   ```
   Page 5: Failed 20 times over 80 minutes
   â†’ âš ï¸  API completely down. Move to previous day.
   ```

---

## ðŸ’¾ **What About Individual Contract Failures?**

### **They're tracked separately:**

```
Page 5: 100 contracts found
  - 95 contracts: âœ… Downloaded successfully
  - 3 contracts: âŒ 500 errors
  - 2 contracts: âŒ Timeouts

Result:
âœ… Page succeeds (95 contracts saved)
âš ï¸ 5 failures logged to fpds_failed_contracts
ðŸ’¾ Move to next page
```

**These individual failures are retried later with:**
- `fpds-retry-failed.ts` script
- Or manually via SQL queries

---

## ðŸš¨ **What Changed From Before:**

### **OLD Behavior (BAD):**
```
Page 1 fails 3x â†’ Skip to Page 2 âŒ
Page 2 fails 3x â†’ Skip to Page 3 âŒ
Page 3 fails 3x â†’ Give up, next day âŒ

Result: Lost ~300 contracts per day!
```

### **NEW Behavior (GOOD):**
```
Page 1 fails â†’ Retry with backoff (up to 20x)
Page 1 succeeds â†’ Move to Page 2
Page 2 fails â†’ Retry with backoff (up to 20x)
Page 2 succeeds â†’ Move to Page 3
...
Page 11 has 71 contracts â†’ Day complete!

Result: Captured ALL data for the day!
```

---

## ðŸ“ˆ **Expected Outcomes:**

### **Scenario 1: API is flaky (common)**
```
Some pages fail once or twice, then succeed
â†’ âœ… All data captured
â†’ â±ï¸ Takes slightly longer (extra cooldowns)
â†’ ðŸ’¾ 100% data coverage
```

### **Scenario 2: API has sustained issues**
```
Pages take 5-10 retries each
â†’ âœ… Still captures data eventually
â†’ â±ï¸ Takes much longer
â†’ ðŸ’¾ Still 100% data coverage
```

### **Scenario 3: API is completely down**
```
Page fails all 20 attempts (80 minutes)
â†’ âš ï¸ Gives up on that day
â†’ â±ï¸ Moves to next day
â†’ ðŸ“‹ Failed page logged for later retry
```

---

## ðŸ” **Monitoring:**

### **Watch for these patterns:**

**GOOD:**
```
[2025-10-29:P5] âœ… SUCCESS â†’ 100 contracts
[2025-10-29:P6] âŒ Fail â†’ Retry
[2025-10-29:P6] âœ… SUCCESS â†’ 100 contracts
[2025-10-29:P7] âœ… SUCCESS â†’ 71 contracts â† END
```

**ACCEPTABLE:**
```
[2025-10-29:P3] âŒ Fail â†’ Cool 30s â†’ Retry
[2025-10-29:P3] âŒ Fail â†’ Cool 1min â†’ Retry
[2025-10-29:P3] âœ… SUCCESS â†’ 100 contracts
```

**BAD (Rare):**
```
[2025-10-29:P1] âŒ Attempt 18/20
[2025-10-29:P1] âŒ Attempt 19/20
[2025-10-29:P1] âŒ Attempt 20/20
[2025-10-29:P1] âŒ Page failed after 20 attempts
â†’ API is completely down, try again later
```

---

## ðŸŽ¯ **Benefits:**

| Feature | OLD | NEW |
|---------|-----|-----|
| **Skip pages?** | âŒ Yes (after 3 fails) | âœ… No! Retry up to 20x |
| **Exponential backoff?** | âŒ No (fixed 30s) | âœ… Yes (30s â†’ 5min) |
| **Natural end detection?** | âŒ No | âœ… Yes (<100 contracts) |
| **Data loss?** | âŒ High | âœ… Minimal |
| **Resilience?** | âš ï¸ Low | âœ… Very High |

---

## ðŸ”„ **When to Restart:**

The new code is already deployed. To use it:

### **Option 1: Let Current Scraper Finish**
- It will complete its current run with old logic
- Next restart will use new logic

### **Option 2: Restart Now**
```bash
# In tmux
Ctrl+C

# Restart with new logic
./run-fpds-page-level.sh

# Detach
Ctrl+b then d
```

---

## ðŸ“‹ **Summary:**

âœ… **Never skips pages** - Retries up to 20x with exponential backoff  
âœ… **Finds natural end** - Only moves to next day when page has <100 contracts  
âœ… **Persistent** - Waits up to 80 minutes per page if needed  
âœ… **Smart cooldowns** - 30s â†’ 1min â†’ 2min â†’ 4min â†’ 5min (max)  
âœ… **Tracks failures** - Individual contract errors logged separately  

**Bottom Line:** The scraper will now fight much harder to get complete data before giving up! ðŸš€


#!/usr/bin/env node
/**
 * Check FPDS Scraper Status
 * Shows what's been scraped recently
 */

import 'dotenv/config';
import { createClient } from '@supabase/supabase-js';

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

async function checkStatus() {
  console.log('================================================================================');
  console.log('FPDS SCRAPER STATUS CHECK');
  console.log('================================================================================\n');

  // Total contracts
  const { count: totalCount } = await supabase
    .from('fpds_contracts')
    .select('*', { count: 'exact', head: true });

  console.log(`üì¶ Total contracts in database: ${totalCount?.toLocaleString() || 0}\n`);

  // Recent page progress
  console.log('üìä Recent Page Progress (Last 7 Days):');
  console.log('‚îÄ'.repeat(80));
  
  const { data: pageProgress } = await supabase
    .from('fpds_page_progress')
    .select('date, page_number, contracts_found, contracts_inserted, status')
    .gte('date', new Date(Date.now() - 7 * 24 * 60 * 60 * 1000).toISOString().split('T')[0])
    .order('date', { ascending: false })
    .order('page_number', { ascending: false })
    .limit(20);

  if (pageProgress && pageProgress.length > 0) {
    const byDate = pageProgress.reduce((acc: any, p: any) => {
      if (!acc[p.date]) {
        acc[p.date] = { maxPage: 0, totalFound: 0, totalInserted: 0, pages: 0 };
      }
      acc[p.date].maxPage = Math.max(acc[p.date].maxPage, p.page_number);
      acc[p.date].totalFound += p.contracts_found || 0;
      acc[p.date].totalInserted += p.contracts_inserted || 0;
      acc[p.date].pages++;
      return acc;
    }, {});

    Object.entries(byDate).forEach(([date, stats]: [string, any]) => {
      console.log(`${date}: Page ${stats.maxPage}, Found ${stats.totalFound}, Inserted ${stats.totalInserted}`);
    });
  } else {
    console.log('No recent page progress found');
  }

  console.log('\n================================================================================');
  console.log('DIAGNOSIS:');
  console.log('================================================================================\n');

  console.log('‚úÖ Awards endpoint works (verified: 10,000+ contracts per day available)');
  console.log('‚úÖ Scraper pagination works (saw pages 21-24 processing)');
  console.log('‚ùå Transactions endpoint returns 422 error (not supported as expected)');
  console.log('\n‚ö†Ô∏è  ISSUE: Vercel cron timeout');
  console.log('   - Vercel allows 5 minutes (300s)');
  console.log('   - 10,000 contracts √ó 0.5s = 5000s (83 minutes)');
  console.log('   - Cron job WILL timeout before completing a full day\n');

  console.log('üí° SOLUTION:');
  console.log('   1. Daily cron scrapes what it can in 5 minutes (~600 contracts)');
  console.log('   2. Uses resume logic - picks up where it left off next run');
  console.log('   3. Full day takes ~17 cron runs (17 √ó 5min = 85 minutes total)');
  console.log('   4. Historical scraper runs locally (no timeout) for backfill\n');

  console.log('================================================================================\n');
}

checkStatus().catch(console.error);

